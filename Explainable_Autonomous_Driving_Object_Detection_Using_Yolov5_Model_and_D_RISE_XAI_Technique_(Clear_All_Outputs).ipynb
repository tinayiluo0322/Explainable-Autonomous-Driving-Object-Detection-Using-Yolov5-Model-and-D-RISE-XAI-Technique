{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinayiluo0322/Explainable-Autonomous-Driving-Object-Detection-Using-Yolov5-Model-and-D-RISE-XAI-Technique/blob/main/Explainable_Autonomous_Driving_Object_Detection_Using_Yolov5_Model_and_D_RISE_XAI_Technique_(Clear_All_Outputs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl6vUDAkuPmE"
      },
      "source": [
        "# AIPI 590 - XAI | XAI with Yolov5 for Object Detection in Autonomous Driving\n",
        "\n",
        "### Explainable Autonomous Driving Object Detection Using Yolov5 Model and D-RISE XAI Technique\n",
        "### Luopeiwen Yi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook delves into the interpretability of object detection models, specifically within the context of autonomous driving. In this study, I use the YOLOv5 model, a state-of-the-art object detection algorithm, to analyze two sample images from the widely used KITTI dataset, which captures real-world driving scenarios, including various objects like cars and pedestrians. The KITTI dataset provides a diverse and challenging environment, making it ideal for evaluating object detection models in autonomous driving tasks.\n",
        "\n",
        "The notebook is structured into two main phases. First, I test the performance of a pre-trained YOLOv5 model by running inference on the two selected images. This initial evaluation assesses YOLOv5’s ability to accurately detect and localize key objects in complex driving scenes. Next, I apply the D-RISE (Detector Randomized Input Sampling for Explanation) technique to generate visual explanations for YOLOv5's predictions on these images. D-RISE works by masking random parts of the input images and measuring the impact on detection confidence, ultimately producing heatmaps that reveal the most influential regions for each detected object.\n",
        "\n",
        "By combining YOLOv5’s detection capabilities with the interpretability offered by D-RISE, this notebook provides a comprehensive analysis of the model’s decision-making process. The goal is to enhance transparency and trust in object detection models used in autonomous vehicles, offering insights into how the model prioritizes different regions of an image during detection. This can lead to improved safety and reliability in real-world driving applications, making the integration of explainable AI techniques critical in autonomous driving systems."
      ],
      "metadata": {
        "id": "UHH-UcA3lO8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJMDF9ppzW2M"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohRLm1D-LWRP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import json\n",
        "!pip install lime\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "!pip install Pillow\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "!pip install shap\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install alibi\n",
        "from alibi.explainers import AnchorImage\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "import random\n",
        "import json\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import shap\n",
        "import cv2\n",
        "!pip install utils\n",
        "from utils.general import non_max_suppression, box_iou\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnKixiYmLXEU"
      },
      "source": [
        "Check GPU Availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4IzESZ2K0II"
      },
      "outputs": [],
      "source": [
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Seed"
      ],
      "metadata": {
        "id": "EdLNff7aHdkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a global random seed\n",
        "GLOBAL_SEED = 42\n",
        "\n",
        "def set_global_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set the global seed\n",
        "set_global_seed(GLOBAL_SEED)"
      ],
      "metadata": {
        "id": "qzOhXTwHHdE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBb-cCV7K315"
      },
      "source": [
        "Access Original KITTI Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rvF8TXA6Siz"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to the KITTI dataset folder in Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/KITTI_Dataset/'\n",
        "\n",
        "# List the files in the dataset folder to confirm upload\n",
        "if os.path.isdir(dataset_path):\n",
        "    print(f\"Dataset directory contents: {os.listdir(dataset_path)}\")\n",
        "else:\n",
        "    print(f\"Directory {dataset_path} not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqG0DA47Vqri"
      },
      "source": [
        "## Pre-trained YoloV5 Performance on Sample KITTI Dataset Image\n",
        "\n",
        "Since We are just using the pre-trained YoloV5, we are going to directly test it's performance on the smaple images from the KITTI's original training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyBJFfiXoO5W"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jaxxDHh3ip1"
      },
      "source": [
        "Image 007480"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pZ0cqHpwyNK"
      },
      "outputs": [],
      "source": [
        "# Load the image using OpenCV\n",
        "image_path_007480 = '/content/drive/MyDrive/KITTI_Dataset/original_training/training_images/007480.png'\n",
        "image_007480 = cv2.imread(image_path_007480)\n",
        "\n",
        "# Display the image\n",
        "cv2_imshow(image_007480)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBee7H7Wxk7z"
      },
      "outputs": [],
      "source": [
        "# Convert the image to a PyTorch format using model input processing\n",
        "results_007480 = model(image_007480)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zyCHmgLq-T9"
      },
      "outputs": [],
      "source": [
        "# Run the detection on a sample image with custom output location\n",
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/KITTI_Dataset/original_training/training_images/007480.png --project /content/drive/MyDrive/KITTI_Dataset/sample_detection --name yolov5_results --exist-ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEGfNYmq1skg"
      },
      "outputs": [],
      "source": [
        "# List all files in the detection result folder to find the correct image\n",
        "output_dir = '/content/drive/MyDrive/KITTI_Dataset/sample_detection/yolov5_results'\n",
        "print(os.listdir(output_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82eiCIuPq77X"
      },
      "outputs": [],
      "source": [
        "# Update the path to the correct folder where YOLOv5 saved the detection results\n",
        "display(Image(filename='/content/drive/MyDrive/KITTI_Dataset/sample_detection/yolov5_results/007480.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3T2PXeO3lo_"
      },
      "source": [
        "Image 000015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxeBkdai3JT6"
      },
      "outputs": [],
      "source": [
        "# Load the image using OpenCV\n",
        "image_path_000015 = '/content/drive/MyDrive/KITTI_Dataset/original_training/training_images/000015.png'\n",
        "image_000015 = cv2.imread(image_path_000015)\n",
        "\n",
        "# Display the image\n",
        "cv2_imshow(image_000015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s69Sq8tW4iYs"
      },
      "outputs": [],
      "source": [
        "# Convert the image to a PyTorch format using model input processing\n",
        "results_000015 = model(image_000015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY3fjAX04xFG"
      },
      "outputs": [],
      "source": [
        "# Run the detection on a sample image with custom output location\n",
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/KITTI_Dataset/original_training/training_images/000015.png --project /content/drive/MyDrive/KITTI_Dataset/sample_detection --name yolov5_results --exist-ok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrsccP3a5RbF"
      },
      "outputs": [],
      "source": [
        "# List all files in the detection result folder to find the correct image\n",
        "output_dir = '/content/drive/MyDrive/KITTI_Dataset/sample_detection/yolov5_results'\n",
        "print(os.listdir(output_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "704dEAXM5Wl3"
      },
      "outputs": [],
      "source": [
        "# Update the path to the correct folder where YOLOv5 saved the detection results\n",
        "display(Image(filename='/content/drive/MyDrive/KITTI_Dataset/sample_detection/yolov5_results/000015.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XAI Technique with D-RISE"
      ],
      "metadata": {
        "id": "wYzhg-kfVrHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D-RISE\n",
        "\n",
        "[Black-box Explanation of Object Detectors via Saliency Maps](https://arxiv.org/pdf/2006.03204)\n",
        "\n",
        "[D-RISE usage demo with Faster R-CNN](https://github.com/hysts/pytorch_D-RISE/blob/main/demo.ipynb)\n",
        "\n",
        "D-RISE (Detector Randomized Input Sampling for Explanation) is an explainability technique specifically designed for object detection models. It builds upon the original RISE method, which creates visual explanations for image classifiers by analyzing how randomly masked versions of the input image affect the model’s output. D-RISE extends this concept to object detection by generating heatmaps that highlight the importance of different regions in an image for detecting specific objects. By randomly masking parts of the input and observing the resulting changes in detection confidence, D-RISE provides insights into which parts of the image are most influential for each detected object.\n",
        "\n",
        "Here we implement this technique to help explain the decisions made by YOLOv5, aiming to make the models more interpretable and trustworthy for critical applications like autonomous driving."
      ],
      "metadata": {
        "id": "St9fANfjn3pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the Dataset Label for Visualization"
      ],
      "metadata": {
        "id": "TiB_V4ayfo82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_mapping = {\n",
        "    0: 'Pedestrian', 1: 'Cyclist', 2: 'Car'\n",
        "}"
      ],
      "metadata": {
        "id": "Y3IMrIA_fpb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application of D-RISE on Image 007480"
      ],
      "metadata": {
        "id": "gPTqtkDuWDLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_007480 = cv2.cvtColor(image_007480, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "cv2_imshow(image_007480)"
      ],
      "metadata": {
        "id": "eP4kzP_opgWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5 Inference\n",
        "results = model(image_007480)\n",
        "detections = results.xywh[0]  # Extract bounding boxes (xywh format) and confidence\n",
        "\n",
        "# Convert YOLOv5 bounding boxes (x_center, y_center, width, height) to (xmin, ymin, xmax, ymax)\n",
        "def yolo_to_corner(bbox):\n",
        "    x_center, y_center, width, height = bbox[:4]\n",
        "    xmin = x_center - (width / 2)\n",
        "    ymin = y_center - (height / 2)\n",
        "    xmax = x_center + (width / 2)\n",
        "    ymax = y_center + (height / 2)\n",
        "    return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "# D-RISE mask generation\n",
        "def generate_mask(image_size, grid_size, prob_thresh):\n",
        "    image_w, image_h = image_size\n",
        "    grid_w, grid_h = grid_size\n",
        "    cell_w, cell_h = np.ceil(image_w / grid_w).astype(int), np.ceil(image_h / grid_h).astype(int)\n",
        "    up_w, up_h = (grid_w + 1) * cell_w, (grid_h + 1) * cell_h\n",
        "\n",
        "    mask = (np.random.uniform(0, 1, size=(grid_h, grid_w)) < prob_thresh).astype(np.float32)\n",
        "    mask = cv2.resize(mask, (up_w, up_h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    offset_w = np.random.randint(0, cell_w)\n",
        "    offset_h = np.random.randint(0, cell_h)\n",
        "    mask = mask[offset_h:offset_h + image_h, offset_w:offset_w + image_w]\n",
        "    return mask\n",
        "\n",
        "# Apply mask to image\n",
        "def mask_image(image, mask):\n",
        "    return ((image.astype(np.float32) / 255 * np.dstack([mask] * 3)) * 255).astype(np.uint8)\n",
        "\n",
        "# IOU Function\n",
        "def iou(box1, box2):\n",
        "    box1 = np.asarray(box1)\n",
        "    box2 = np.asarray(box2)\n",
        "    tl = np.maximum(box1[:2], box2[:2])\n",
        "    br = np.minimum(box1[2:], box2[2:])\n",
        "    intersection = np.prod(np.maximum(br - tl, 0))\n",
        "    area1 = np.prod(box1[2:] - box1[:2])\n",
        "    area2 = np.prod(box2[2:] - box2[:2])\n",
        "    return intersection / (area1 + area2 - intersection)\n",
        "\n",
        "def generate_saliency_map(image, target_box, prob_thresh=0.5, grid_size=(16, 16), n_masks=1000, seed=0):\n",
        "    np.random.seed(seed)\n",
        "    image_h, image_w = image.shape[:2]\n",
        "    saliency_map = np.zeros((image_h, image_w), dtype=np.float32)\n",
        "\n",
        "    for _ in tqdm(range(n_masks)):\n",
        "        mask = generate_mask(image_size=(image_w, image_h), grid_size=grid_size, prob_thresh=prob_thresh)\n",
        "        masked_image = mask_image(image, mask)\n",
        "\n",
        "        # Perform inference on masked image\n",
        "        results_masked = model(masked_image)\n",
        "        preds = results_masked.xywh[0].cpu().numpy()\n",
        "\n",
        "        # Check if the bounding box overlaps with the target_box\n",
        "        score = 0\n",
        "        for pred in preds:\n",
        "            pred_box = yolo_to_corner(pred[:4])\n",
        "            iou_score = iou(target_box, pred_box)\n",
        "            score = max(score, iou_score * pred[4])  # pred[4] is the confidence score\n",
        "\n",
        "        saliency_map += mask * score\n",
        "\n",
        "    return saliency_map"
      ],
      "metadata": {
        "id": "HjIIUbDo8rKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert YOLOv5 bounding boxes to the corner format and move to CPU\n",
        "if len(detections) > 0:\n",
        "    # Create a figure with subplots\n",
        "    n_detections = len(detections)\n",
        "    fig, axes = plt.subplots(n_detections, 3, figsize=(15, 5 * n_detections))\n",
        "    if n_detections == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for idx, detection in enumerate(detections):\n",
        "        target_box = yolo_to_corner(detection[:4].cpu().numpy())\n",
        "        class_id = int(detection[5].item())  # Get the class ID\n",
        "        class_name = class_mapping.get(class_id, f'Unknown_{class_id}')  # Get the class name\n",
        "        confidence = detection[4].item()  # Get the confidence score\n",
        "\n",
        "        # Generate the saliency map for the detected object\n",
        "        saliency_map = generate_saliency_map(image_007480, target_box, prob_thresh=0.5, grid_size=(16, 16), n_masks=1000)\n",
        "\n",
        "        # Normalize saliency map\n",
        "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "        # Overlay the saliency map on the image\n",
        "        image_with_bbox = image_007480.copy()\n",
        "        cv2.rectangle(image_with_bbox, tuple(map(int, target_box[:2])), tuple(map(int, target_box[2:])), (0, 255, 0), 2)\n",
        "\n",
        "        # Add text to the image\n",
        "        label = f'{class_name} {confidence:.2f}'\n",
        "        cv2.putText(image_with_bbox, label, (int(target_box[0]), int(target_box[1] - 10)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        # Original Image with bounding box\n",
        "        axes[idx, 0].imshow(image_with_bbox)\n",
        "        axes[idx, 0].set_title(f'Object {idx+1}: {class_name}')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        # Saliency Map\n",
        "        axes[idx, 1].imshow(saliency_map, cmap='jet')\n",
        "        axes[idx, 1].set_title(f'Object {idx+1}: {class_name} Saliency Map')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Saliency Map Overlay\n",
        "        axes[idx, 2].imshow(image_with_bbox)\n",
        "        axes[idx, 2].imshow(saliency_map, cmap='jet', alpha=0.5)\n",
        "        axes[idx, 2].set_title(f'Object {idx+1}: {class_name} Saliency Map Overlay')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure as an image file\n",
        "    output_path = '/content/drive/MyDrive/KITTI_Dataset/d_rise_output_multiple_007480_with_labels.png'\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Output image saved to: {output_path}\")\n",
        "    print(f\"Number of objects detected: {n_detections}\")\n",
        "\n",
        "    # display the saved image\n",
        "    from IPython.display import Image, display\n",
        "    display(Image(filename=output_path))\n",
        "else:\n",
        "    print(\"No detections found in the image.\")"
      ],
      "metadata": {
        "id": "0LFm_1Mr_WGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize the image"
      ],
      "metadata": {
        "id": "iRq5plt9Z22K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_007480_resized = cv2.resize(image_007480, (640, 640))\n",
        "cv2_imshow(image_007480_resized)"
      ],
      "metadata": {
        "id": "fy17HVz6ZR6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5 Inference\n",
        "results = model(image_007480_resized)\n",
        "detections = results.xywh[0]  # Extract bounding boxes (xywh format) and confidence"
      ],
      "metadata": {
        "id": "GwSswAJ1Z8kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert YOLOv5 bounding boxes to the corner format and move to CPU\n",
        "if len(detections) > 0:\n",
        "    # Create a figure with subplots\n",
        "    n_detections = len(detections)\n",
        "    fig, axes = plt.subplots(n_detections, 3, figsize=(15, 5 * n_detections))\n",
        "    if n_detections == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for idx, detection in enumerate(detections):\n",
        "        target_box = yolo_to_corner(detection[:4].cpu().numpy())\n",
        "        class_id = int(detection[5].item())  # Get the class ID\n",
        "        class_name = class_mapping.get(class_id, f'Unknown_{class_id}')  # Get the class name\n",
        "        confidence = detection[4].item()  # Get the confidence score\n",
        "\n",
        "        # Generate the saliency map for the detected object\n",
        "        saliency_map = generate_saliency_map(image_007480_resized, target_box, prob_thresh=0.5, grid_size=(16, 16), n_masks=1000)\n",
        "\n",
        "        # Normalize saliency map\n",
        "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "        # Overlay the saliency map on the image\n",
        "        image_with_bbox = image_007480_resized.copy()\n",
        "        cv2.rectangle(image_with_bbox, tuple(map(int, target_box[:2])), tuple(map(int, target_box[2:])), (0, 255, 0), 2)\n",
        "\n",
        "        # Add text to the image\n",
        "        label = f'{class_name} {confidence:.2f}'\n",
        "        cv2.putText(image_with_bbox, label, (int(target_box[0]), int(target_box[1] - 10)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        # Original Image with bounding box\n",
        "        axes[idx, 0].imshow(image_with_bbox)\n",
        "        axes[idx, 0].set_title(f'Object {idx+1}: {class_name}')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        # Saliency Map\n",
        "        axes[idx, 1].imshow(saliency_map, cmap='jet')\n",
        "        axes[idx, 1].set_title(f'Object {idx+1}: {class_name} Saliency Map')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Saliency Map Overlay\n",
        "        axes[idx, 2].imshow(image_with_bbox)\n",
        "        axes[idx, 2].imshow(saliency_map, cmap='jet', alpha=0.5)\n",
        "        axes[idx, 2].set_title(f'Object {idx+1}: {class_name} Saliency Map Overlay')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure as an image file\n",
        "    output_path = '/content/drive/MyDrive/KITTI_Dataset/d_rise_output_multiple_007480_resize_with_labels.png'\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Output image saved to: {output_path}\")\n",
        "    print(f\"Number of objects detected: {n_detections}\")\n",
        "\n",
        "    # display the saved image\n",
        "    from IPython.display import Image, display\n",
        "    display(Image(filename=output_path))\n",
        "else:\n",
        "    print(\"No detections found in the image.\")"
      ],
      "metadata": {
        "id": "EfBL43oXaPVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application of D-RISE on Image 000015"
      ],
      "metadata": {
        "id": "UrPu3KSaC25z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_000015 = cv2.cvtColor(image_000015, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "cv2_imshow(image_000015)"
      ],
      "metadata": {
        "id": "nJxmPv-jC2tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5 Inference\n",
        "results = model(image_000015)\n",
        "detections = results.xywh[0]  # Extract bounding boxes (xywh format) and confidence"
      ],
      "metadata": {
        "id": "hfc_OcKrKGku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert YOLOv5 bounding boxes to the corner format and move to CPU\n",
        "if len(detections) > 0:\n",
        "    # Create a figure with subplots\n",
        "    n_detections = len(detections)\n",
        "    fig, axes = plt.subplots(n_detections, 3, figsize=(15, 5 * n_detections))\n",
        "    if n_detections == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for idx, detection in enumerate(detections):\n",
        "        target_box = yolo_to_corner(detection[:4].cpu().numpy())\n",
        "        class_id = int(detection[5].item())  # Get the class ID\n",
        "        class_name = class_mapping.get(class_id, f'Unknown_{class_id}')  # Get the class name\n",
        "        confidence = detection[4].item()  # Get the confidence score\n",
        "\n",
        "        # Generate the saliency map for the detected object\n",
        "        saliency_map = generate_saliency_map(image_000015, target_box, prob_thresh=0.5, grid_size=(16, 16), n_masks=1000)\n",
        "\n",
        "        # Normalize saliency map\n",
        "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "        # Overlay the saliency map on the image\n",
        "        image_with_bbox = image_000015.copy()\n",
        "        cv2.rectangle(image_with_bbox, tuple(map(int, target_box[:2])), tuple(map(int, target_box[2:])), (0, 255, 0), 2)\n",
        "\n",
        "        # Add text to the image\n",
        "        label = f'{class_name} {confidence:.2f}'\n",
        "        cv2.putText(image_with_bbox, label, (int(target_box[0]), int(target_box[1] - 10)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        # Original Image with bounding box\n",
        "        axes[idx, 0].imshow(image_with_bbox)\n",
        "        axes[idx, 0].set_title(f'Object {idx+1}: {class_name}')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        # Saliency Map\n",
        "        axes[idx, 1].imshow(saliency_map, cmap='jet')\n",
        "        axes[idx, 1].set_title(f'Object {idx+1}: {class_name} Saliency Map')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Saliency Map Overlay\n",
        "        axes[idx, 2].imshow(image_with_bbox)\n",
        "        axes[idx, 2].imshow(saliency_map, cmap='jet', alpha=0.5)\n",
        "        axes[idx, 2].set_title(f'Object {idx+1}: {class_name} Saliency Map Overlay')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure as an image file\n",
        "    output_path = '/content/drive/MyDrive/KITTI_Dataset/d_rise_output_multiple_000015_with_labels.png'\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Output image saved to: {output_path}\")\n",
        "    print(f\"Number of objects detected: {n_detections}\")\n",
        "\n",
        "# display the saved image\n",
        "    from IPython.display import Image, display\n",
        "    display(Image(filename=output_path))\n",
        "else:\n",
        "    print(\"No detections found in the image.\")"
      ],
      "metadata": {
        "id": "D-zr7hVZKLcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize the Image"
      ],
      "metadata": {
        "id": "5XhS87PIalHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_000015_resized = cv2.resize(image_000015, (640, 640))\n",
        "cv2_imshow(image_000015_resized)"
      ],
      "metadata": {
        "id": "ItqUEesCak1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv5 Inference\n",
        "results = model(image_000015_resized)\n",
        "detections = results.xywh[0]  # Extract bounding boxes (xywh format) and confidence"
      ],
      "metadata": {
        "id": "T8fawv07a1tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert YOLOv5 bounding boxes to the corner format and move to CPU\n",
        "if len(detections) > 0:\n",
        "    # Create a figure with subplots\n",
        "    n_detections = len(detections)\n",
        "    fig, axes = plt.subplots(n_detections, 3, figsize=(15, 5 * n_detections))\n",
        "    if n_detections == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for idx, detection in enumerate(detections):\n",
        "        target_box = yolo_to_corner(detection[:4].cpu().numpy())\n",
        "        class_id = int(detection[5].item())  # Get the class ID\n",
        "        class_name = class_mapping.get(class_id, f'Unknown_{class_id}')  # Get the class name\n",
        "        confidence = detection[4].item()  # Get the confidence score\n",
        "\n",
        "        # Generate the saliency map for the detected object\n",
        "        saliency_map = generate_saliency_map(image_000015_resized, target_box, prob_thresh=0.5, grid_size=(16, 16), n_masks=1000)\n",
        "\n",
        "        # Normalize saliency map\n",
        "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "\n",
        "        # Overlay the saliency map on the image\n",
        "        image_with_bbox = image_000015_resized.copy()\n",
        "        cv2.rectangle(image_with_bbox, tuple(map(int, target_box[:2])), tuple(map(int, target_box[2:])), (0, 255, 0), 2)\n",
        "\n",
        "        # Add text to the image\n",
        "        label = f'{class_name} {confidence:.2f}'\n",
        "        cv2.putText(image_with_bbox, label, (int(target_box[0]), int(target_box[1] - 10)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        # Original Image with bounding box\n",
        "        axes[idx, 0].imshow(image_with_bbox)\n",
        "        axes[idx, 0].set_title(f'Object {idx+1}: {class_name}')\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        # Saliency Map\n",
        "        axes[idx, 1].imshow(saliency_map, cmap='jet')\n",
        "        axes[idx, 1].set_title(f'Object {idx+1}: {class_name} Saliency Map')\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Saliency Map Overlay\n",
        "        axes[idx, 2].imshow(image_with_bbox)\n",
        "        axes[idx, 2].imshow(saliency_map, cmap='jet', alpha=0.5)\n",
        "        axes[idx, 2].set_title(f'Object {idx+1}: {class_name} Saliency Map Overlay')\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure as an image file\n",
        "    output_path = '/content/drive/MyDrive/KITTI_Dataset/d_rise_output_multiple_000015_resize_with_labels.png'\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"Output image saved to: {output_path}\")\n",
        "    print(f\"Number of objects detected: {n_detections}\")\n",
        "\n",
        "    # display the saved image\n",
        "    from IPython.display import Image, display\n",
        "    display(Image(filename=output_path))\n",
        "else:\n",
        "    print(\"No detections found in the image.\")"
      ],
      "metadata": {
        "id": "ZmMtFXI8a7q4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}